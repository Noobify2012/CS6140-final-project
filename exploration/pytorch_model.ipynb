{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from final_project.models import run_model\n",
    "from final_project.models import save_model_pkl\n",
    "from final_project.models import load_model\n",
    "from final_project.models import plot_frequencies\n",
    "import final_project.plots as plots\n",
    "from final_project import builder\n",
    "from final_project.loader import get_df\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_df = get_df(year=\"2018\")\n",
    "master_df = get_df(all_files=True)\n",
    "master_df = master_df[master_df['Origin'] == 'BOS']\n",
    "master_df = master_df[[\n",
    "    'DistanceGroup',\n",
    "    'DayofMonth',\n",
    "    'Month',\n",
    "    'Year',\n",
    "    'Duplicate',\n",
    "    'ArrDel15',\n",
    "    'DistanceGroup',\n",
    "    'WeatherDelay',\n",
    "    'NASDelay',\n",
    "    'SecurityDelay',\n",
    "    'Operating_Airline',\n",
    "    'Dest'\n",
    "]]\n",
    "master_df = builder.columnManager(master_df)\n",
    "master_df = builder.encodeFrame(master_df)\n",
    "# master_df = master_df[[\"date_sin\",\"date_cos\",\"ArrDel15\",\"month_sin\",\"month_cos\",\"DistanceGroup\",\"WeatherDelay\",\"NASDelay\",\"SecurityDelay\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "process data for training, split into test/train/validation\n",
    "'''\n",
    "X = master_df.drop(columns=[\"ArrDel15\"])\n",
    "y = master_df[[\"ArrDel15\"]]\n",
    "y = y.ArrDel15.ravel() # flatten\n",
    "print(\"# samples:\", y.shape[0])\n",
    "\n",
    "# split into train and test/validation (which is then split in next line)\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(X, y,\n",
    "                                                    test_size=0.6,\n",
    "                                                    random_state=150)\n",
    "\n",
    "# create validation and test sets each 15% of total data\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test_val, y_test_val,\n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=150)\n",
    "data = {\"train\": (X_train,y_train), \"test\":(X_test,y_test), \"validation\": (X_validation,y_validation)}\n",
    "\n",
    "# number of classes, number of instances in each class\n",
    "for each in data.keys():\n",
    "    print(f\"{each}:\")\n",
    "    print(\" - Number of features: \", len(data[each][0].columns))\n",
    "    print(\" - Number of samples: \", len(data[each][0]))\n",
    "    unique, counts = np.unique(data[each][1], return_counts=True)\n",
    "    plot_frequencies(each, (unique, counts))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train numpy arrays\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "X_validation = X_validation.to_numpy()\n",
    "\n",
    "# convert to tensors\n",
    "X_train, y_train, X_test, y_test, X_validation, y_validation = map(\n",
    "    torch.tensor, (X_train, y_train, X_test, y_test, X_validation, y_validation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "valid_ds = TensorDataset(X_validation, y_validation)\n",
    "num_features=X_train.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"bs\":(64,),\n",
    "        \"epoch\":(10,),\n",
    "        \"learning_rate\":(.01,),\n",
    "        \"momentum\":(0,),\n",
    "        \"weight_decay\":(0,),\n",
    "        \"activation_fn\": nn.ReLU,\n",
    "        \"dropout_prob\": (0,),\n",
    "        \"num_layers\": (1,),\n",
    "        \"num_nodes\": (2,)}\n",
    "best_model_params, best_model = run_model(param_dict=params, \n",
    "                  train_ds=train_ds, \n",
    "                  test_ds=test_ds, \n",
    "                  valid_ds=valid_ds,\n",
    "                  num_features=num_features\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_labels, y_prob, y_pred = best_model.predict(test_ds)\n",
    "plots.save_precision_recall_curve(\"ffn\", \"ffn_1_2_prec_rec_curve\", y_test, y_prob)\n",
    "plots.save_confusion_matrix(\"ffn\", \"ffn_1_2_confusion\", y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_pkl(best_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
