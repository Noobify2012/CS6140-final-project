{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from final_project.loader import get_df\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_freq(title, data):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6,3))\n",
    "    bars = ax.bar(['delayed' if x==1 else 'on time' for x in data[0]], data[1])\n",
    "\n",
    "    ax.set_title(f'Class Instances in {title.capitalize()} Dataset')\n",
    "    ax.set_xlabel('Count')\n",
    "    ax.set_ylabel('Labels')\n",
    "\n",
    "    print(data[1])\n",
    "    ax.bar_label(bars, data[1])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # Show the chart\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = get_df(file=\"Flights_2018_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "process data for training, split into test/train/validation\n",
    "'''\n",
    "X = master_df.drop(columns=[\"ArrDel15\",\"Duplicate\"])\n",
    "y = master_df[[\"ArrDel15\"]]\n",
    "y = y.ArrDel15.ravel() # flatten\n",
    "\n",
    "print(\"# samples:\", y.shape[0])\n",
    "\n",
    "# split into train and test/validation (which is split in next line)\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=150)\n",
    "\n",
    "# create validation and test sets each 15% of total data\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test_val, y_test_val,\n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=150)\n",
    "d = {\"train\": (X_train,y_train), \"test\":(X_test,y_test), \"validation\": (X_validation,y_validation)}\n",
    "\n",
    "\n",
    "# number of classes, number of instances in each class\n",
    "for each in d.keys():\n",
    "    print(f\"{each}:\")\n",
    "    print(\" - Number of features: \", len(d[each][0].columns))\n",
    "    print(\" - Number of samples: \", len(d[each][0]))\n",
    "    unique, counts = np.unique(d[each][1], return_counts=True)\n",
    "    plot_freq(each, (unique, counts))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train numpy arrays\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "X_validation = X_validation.to_numpy()\n",
    "\n",
    "# convert to tensors\n",
    "X_train, y_train, X_test, y_test, X_validation, y_validation = map(\n",
    "    torch.tensor, (X_train, y_train, X_test, y_test, X_validation, y_validation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model including fit and score methods\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, num_nodes:int, num_features) -> None:\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.num_classes = 2\n",
    "        if num_hidden_layers == 1:\n",
    "             self.layers = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(num_features, num_nodes),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_nodes, 1),\n",
    "            )\n",
    "        else:\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(num_features, num_nodes),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_nodes, num_nodes),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_nodes, 1),\n",
    "            )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.to(self.layers[1].weight.dtype)\n",
    "        return self.layers(xb)\n",
    "        \n",
    "    # TODO add momentum\n",
    "    def fit(self, train_dataset: TensorDataset, validation_dataset: TensorDataset, batch_size: int, \n",
    "            epochs: int, loss_function, learning_rate: float):\n",
    "        \n",
    "        # create dataloader for batching, shuffle to avoid overfitting/batch correlation\n",
    "        train_dl = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "        valid_dl = DataLoader(validation_dataset, batch_size, shuffle=True)\n",
    "\n",
    "        # opt = torch.optim.SGD(self.parameters(), lr=learning_rate, momentum=.9, weight_decay=.0000001) # create optimizer TODO weight decay\n",
    "        # TODO tune optimizer\n",
    "        # opt = torch.optim.SGD(self.parameters(), lr=learning_rate) # create optimizer TODO weight decay\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        # store epoch losses\n",
    "        training_losses = []\n",
    "        validation_losses = []\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            self.train()\n",
    "            epoch_training_loss = 0\n",
    "            epoch_validation_loss = 0\n",
    "\n",
    "            for xb, yb in train_dl:\n",
    "                # run model on batch and get loss\n",
    "                predictions = self(xb).squeeze()\n",
    "                loss = loss_function(predictions, yb)\n",
    "                # Back Propagation\n",
    "                loss.backward()  # compute gradient\n",
    "                opt.step()       # update weights\n",
    "                opt.zero_grad()  # reset gradient\n",
    "\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for xb_tr, yb_tr in train_dl:\n",
    "                    # train loss\n",
    "                    train_predictions = self(xb_tr).squeeze()\n",
    "                    training_loss = loss_function(train_predictions, yb_tr)\n",
    "                    epoch_training_loss += (training_loss * xb_tr.shape[0])\n",
    "                for xb_val, yb_val in valid_dl:\n",
    "                    # val loss\n",
    "                    val_predictions = self(xb_val).squeeze()\n",
    "                    validation_loss = loss_function(val_predictions, yb_val) # get loss\n",
    "                    epoch_validation_loss += (validation_loss * xb_val.shape[0])\n",
    "                # get epoch loss\n",
    "                num_train_samples = len(train_dataset)\n",
    "                epoch_training_loss_normalized = epoch_training_loss / num_train_samples \n",
    "                training_losses.append(epoch_training_loss_normalized.item())\n",
    "\n",
    "                num_val_samples = len(validation_dataset)\n",
    "                epoch_validation_loss_normalized = epoch_validation_loss / (num_val_samples) \n",
    "                validation_losses.append(epoch_validation_loss_normalized.item())\n",
    "\n",
    "        return training_losses, validation_losses\n",
    "\n",
    "    def score(self, tensor_dataset: TensorDataset, batch_size=1) -> float:\n",
    "        # reference: https://blog.paperspace.com/training-validation-and-accuracy-in-pytorch/\n",
    "        self.eval()\n",
    "        dataloader = DataLoader(tensor_dataset, batch_size, shuffle=True)\n",
    "        all_predictions = torch.tensor([],dtype=torch.long)\n",
    "        ground_truth_labels = torch.tensor([],dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in tqdm(dataloader):\n",
    "                # run model on batch\n",
    "                class_probabilities = self(xb)\n",
    "                \n",
    "                # choose most likely class for each sample\n",
    "                predictions = (class_probabilities > 0.5).long().squeeze()\n",
    "                \n",
    "                # create running tensor with all true_label, predicted_label pairs \n",
    "                ground_truth_labels = torch.cat((ground_truth_labels, yb))\n",
    "                all_predictions = torch.cat((all_predictions, predictions))\n",
    "            \n",
    "            # create a stack where the top layer is the ground truth, bottom is prediction\n",
    "            true_pred_stack = torch.stack((ground_truth_labels, all_predictions))\n",
    "            classifier_scores = precision_recall_fscore_support(ground_truth_labels, all_predictions)\n",
    "            ConfusionMatrixDisplay.from_predictions(ground_truth_labels, all_predictions)\n",
    "            confusion_mtx = confusion_matrix(ground_truth_labels, all_predictions)\n",
    "            class_accuracy = {i:0 for i in range(self.num_classes)} # dictionary containing class accuracies\n",
    "            for label in range(self.num_classes):\n",
    "                # get all pairs with the same true label in the tensor stack\n",
    "                class_pairs = list(filter(lambda pair: pair[0] == label, true_pred_stack.T)) \n",
    "\n",
    "                # calculate how many have correctly been predicted (true = predicted)\n",
    "                num_correct = len(list(filter(lambda pair: pair[0] == pair[1], class_pairs)))\n",
    "\n",
    "                # find accuracy\n",
    "                class_accuracy[label] = num_correct/len(class_pairs)\n",
    "                \n",
    "            # get mean accuracy\n",
    "            correct = sum(all_predictions == ground_truth_labels).item()\n",
    "            mean_accuracy = correct / len(tensor_dataset)\n",
    "            \n",
    "            return mean_accuracy, class_accuracy, classifier_scores, confusion_mtx\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "valid_ds = TensorDataset(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "num_features = X_train.shape[1]\n",
    "classes = y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exhaustive hyperparameter tuning based on the best final validation loss\n",
    "def ffn_tune(num_hidden_layers, num_nodes):\n",
    "    params = {\"bs\":(64,),\n",
    "                \"epoch\":(50,),\n",
    "                \"learning_rate\":(.01,)}\n",
    "    # params = {\"bs\":(50, 64, 80),\n",
    "    #             \"epoch\":(30,50),\n",
    "    #             \"learning_rate\":(.008,.09)}\n",
    "    # params = {\"bs\":(30,),\n",
    "    #             \"epoch\":(50,),\n",
    "    #             \"learning_rate\":(.01, .05, .09, .1, .3, .5)}\n",
    "    best_model = {key:params[key] for key in params}\n",
    "    best_model[\"best_loss\"] = 100000000000\n",
    "    for bs in params[\"bs\"]:\n",
    "        for epoch in params[\"epoch\"]:\n",
    "            for lr in params[\"learning_rate\"]:\n",
    "                # use validation loss\n",
    "                model = FeedForward(num_hidden_layers, num_nodes, num_features)\n",
    "                training_losses, valid_losses = model.fit(train_ds, valid_ds, bs, epoch, loss_function, lr)\n",
    "                if valid_losses[-1] < best_model[\"best_loss\"]:\n",
    "                    best_model[\"model\"]=model\n",
    "                    best_model[\"best_loss\"] = valid_losses[-1]\n",
    "                    best_model[\"epoch\"] = epoch\n",
    "                    best_model[\"learning_rate\"] = lr\n",
    "                    best_model[\"bs\"] = bs\n",
    "                    best_model[\"valid_losses\"] = valid_losses\n",
    "                    best_model[\"training_losses\"] = training_losses\n",
    "                print(\"best loss: \",best_model[\"best_loss\"])\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffn_evaluate(model):\n",
    "# plot losses\n",
    "    plt.plot(model[\"training_losses\"], label=\"Training Loss\")\n",
    "    # print(model[\"training_losses\"])\n",
    "    plt.plot(model[\"valid_losses\"], label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # calculate accuracy\n",
    "    d = {\"train\": train_ds, \"test\": test_ds, \"validation\": valid_ds}\n",
    "    for dataset in d:\n",
    "        print(f\"Evaluating **{dataset}** dataset:\")\n",
    "        mean_accuracy, class_accuracy, classifier_scores, confusion_matrix = model[\"model\"].score(d[dataset], model[\"bs\"])\n",
    "        print(f\"Mean Accuracy: {mean_accuracy*100:.3f}\")\n",
    "        print(f\"Mean per-class accuracy:\")\n",
    "        for key in class_accuracy:\n",
    "            print(f\"  {'delayed' if key==1 else 'on time'}{': '}{class_accuracy[key]*100:.3f}%\")\n",
    "        print(f\"Precision: {classifier_scores[0]}\")\n",
    "        print(f\"Recall: {classifier_scores[1]}\")\n",
    "        print(f\"F-Beta Score: {classifier_scores[2]}\")\n",
    "        print(f\"F1 Score: {classifier_scores}\")\n",
    "        print(confusion_matrix)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model tuning and evaluation on the 4 combos of layers/nodes\n",
    "def run_model(num_layers: int, num_nodes: int):\n",
    "    print(f\"{num_nodes} Nodes, {num_layers} Hidden Layer(s)\")\n",
    "    model = ffn_tune(num_layers,num_nodes)\n",
    "    print(\"best batch size: \", model[\"bs\"])\n",
    "    print(\"best epoch: \", model[\"epoch\"])\n",
    "    print(\"best learning rate: \", model[\"learning_rate\"])\n",
    "    ffn_evaluate(model)\n",
    "\n",
    "for pair in [(1,4)]:\n",
    "    run_model(pair[0],pair[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
