{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from final_project.models import FeedForward\n",
    "from final_project import builder\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from final_project.loader import get_df\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_freq(title, data):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6,3))\n",
    "    bars = ax.bar(['delayed' if x==1 else 'on time' for x in data[0]], data[1])\n",
    "\n",
    "    ax.set_title(f'Class Instances in {title.capitalize()} Dataset')\n",
    "    ax.set_xlabel('Count')\n",
    "    ax.set_ylabel('Labels')\n",
    "\n",
    "    print(data[1])\n",
    "    ax.bar_label(bars, data[1])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # Show the chart\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = get_df(year=\"2018\")\n",
    "master_df = master_df.sample(n=100000, random_state=150)\n",
    "# builder.runEDA(master_df)\n",
    "master_df = builder.columnManager(master_df)\n",
    "master_df = builder.encodeFrame(master_df)\n",
    "master_df = master_df[[\"date_sin\",\"date_cos\",\"ArrDel15\",\"month_sin\",\"month_cos\",\"DistanceGroup\",\"WeatherDelay\",\"NASDelay\",\"SecurityDelay\"]]\n",
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "process data for training, split into test/train/validation\n",
    "'''\n",
    "X = master_df.drop(columns=[\"ArrDel15\"])\n",
    "y = master_df[[\"ArrDel15\"]]\n",
    "y = y.ArrDel15.ravel() # flatten\n",
    "print(\"# samples:\", y.shape[0])\n",
    "\n",
    "# split into train and test/validation (which is then split in next line)\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(X, y,\n",
    "                                                    test_size=0.6,\n",
    "                                                    random_state=150)\n",
    "\n",
    "# create validation and test sets each 15% of total data\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test_val, y_test_val,\n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=150)\n",
    "data = {\"train\": (X_train,y_train), \"test\":(X_test,y_test), \"validation\": (X_validation,y_validation)}\n",
    "\n",
    "# number of classes, number of instances in each class\n",
    "for each in data.keys():\n",
    "    print(f\"{each}:\")\n",
    "    print(\" - Number of features: \", len(data[each][0].columns))\n",
    "    print(\" - Number of samples: \", len(data[each][0]))\n",
    "    unique, counts = np.unique(data[each][1], return_counts=True)\n",
    "    plot_freq(each, (unique, counts))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "# # split into train/test sets\n",
    "# X_train, X_test_val, y_train, y_test_val = train_test_split(X, y,\n",
    "#                                                     test_size=0.5,\n",
    "#                                                     random_state=150)\n",
    "\n",
    "# # create validation and test sets each 15% of total data\n",
    "# X_test, X_validation, y_test, y_validation = train_test_split(X_test_val, y_test_val,\n",
    "#                                                     test_size=0.5,\n",
    "#                                                     random_state=150)\n",
    "# # convert to tensors\n",
    "# X_train, y_train, X_test, y_test, X_validation, y_validation = map(\n",
    "#     torch.tensor, (X_train, y_train, X_test, y_test, X_validation, y_validation)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.head())\n",
    "print(X_test.head())\n",
    "print(X_validation.head())\n",
    "\n",
    "# create train numpy arrays\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "X_validation = X_validation.to_numpy()\n",
    "\n",
    "# convert to tensors\n",
    "X_train, y_train, X_test, y_test, X_validation, y_validation = map(\n",
    "    torch.tensor, (X_train, y_train, X_test, y_test, X_validation, y_validation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "valid_ds = TensorDataset(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exhaustive hyperparameter tuning based on the best final validation loss\n",
    "def ffn_tune(num_hidden_layers, num_nodes, param_dict, loss_function, num_features):\n",
    "    best_model = {key:param_dict[key] for key in param_dict}\n",
    "    best_model[\"best_loss\"] = 100000000000\n",
    "    for bs in param_dict[\"bs\"]:\n",
    "        for epoch in param_dict[\"epoch\"]:\n",
    "            for lr in param_dict[\"learning_rate\"]:\n",
    "                # use validation loss\n",
    "                model = FeedForward(num_hidden_layers, num_nodes, num_features, param_dict[\"activation_fn\"])\n",
    "                training_losses, valid_losses = model.fit(train_ds, valid_ds, bs, epoch, loss_function, lr)\n",
    "                if valid_losses[-1] < best_model[\"best_loss\"]:\n",
    "                    best_model[\"model\"]=model\n",
    "                    best_model[\"best_loss\"] = valid_losses[-1]\n",
    "                    best_model[\"epoch\"] = epoch\n",
    "                    best_model[\"learning_rate\"] = lr\n",
    "                    best_model[\"bs\"] = bs\n",
    "                    best_model[\"valid_losses\"] = valid_losses\n",
    "                    best_model[\"training_losses\"] = training_losses\n",
    "                print(\"best loss: \", best_model[\"best_loss\"])\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffn_evaluate(model):\n",
    "# plot losses\n",
    "    plt.plot(model[\"training_losses\"], label=\"Training Loss\")\n",
    "    # print(model[\"training_losses\"])\n",
    "    plt.plot(model[\"valid_losses\"], label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # calculate accuracy\n",
    "    d = {\"train\": train_ds, \"test\": test_ds, \"validation\": valid_ds}\n",
    "    for dataset in d:\n",
    "        print(f\"Evaluating **{dataset}** dataset:\")\n",
    "        mean_accuracy, class_accuracy, classifier_scores, confusion_matrix = model[\"model\"].score(d[dataset], model[\"bs\"])\n",
    "        print(f\"Mean Accuracy: {mean_accuracy*100:.3f}\")\n",
    "        print(f\"Mean per-class accuracy:\")\n",
    "        for key in class_accuracy:\n",
    "            print(f\"  {'delayed' if key==1 else 'on time'}{': '}{class_accuracy[key]*100:.3f}%\")\n",
    "        print(classifier_scores)\n",
    "        print(confusion_matrix)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model tuning and evaluation on the 4 combos of layers/nodes\n",
    "def run_model(param_dict):\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "    num_features = X_train.shape[1]\n",
    "\n",
    "    num_nodes_list = param_dict[\"num_nodes\"]\n",
    "    num_layers_list = param_dict[\"num_layers\"]\n",
    "    for num_nodes in num_nodes_list:\n",
    "        for num_layers in num_layers_list:\n",
    "            print(f\"{num_nodes} Nodes, {num_layers} Hidden Layer(s)\")\n",
    "            best_model = ffn_tune(num_layers,num_nodes, param_dict, loss_function, num_features)\n",
    "            print(\"best batch size: \", best_model[\"bs\"])\n",
    "            print(\"best epoch: \", best_model[\"epoch\"])\n",
    "            print(\"best learning rate: \", best_model[\"learning_rate\"])\n",
    "            ffn_evaluate(best_model)\n",
    "\n",
    "    # TODO save model\n",
    "    # state = best_model[\"model\"].state_dict() # save the model\n",
    "# bs, epoch, learning_rate, momentum, activation function, number layers, number of nodes per hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"bs\":(64,),\n",
    "        \"epoch\":(50,),\n",
    "        \"learning_rate\":(.01,),\n",
    "        \"activation_fn\": nn.ReLU,\n",
    "        \"num_layers\": (1,2),\n",
    "        \"num_nodes\": (2,4,8)}\n",
    "run_model(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
